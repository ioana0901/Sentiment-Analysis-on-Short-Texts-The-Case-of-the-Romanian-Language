{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f3507aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from FunctionsList import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers, callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, GRU, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Bidirectional, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification#, BertTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e256926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>comandat telefon titlu urgență distrus huawei ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>stele inima telefon funcționează impecabil ara...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>multumit telefonul functioneaza impecabil inta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>zile varianta gb gb stocare achizitionat pret ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>senzorul luminozitate nu funcționează ecranul ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                     review_content\n",
       "0      0  comandat telefon titlu urgență distrus huawei ...\n",
       "1      2  stele inima telefon funcționează impecabil ara...\n",
       "2      2  multumit telefonul functioneaza impecabil inta...\n",
       "3      2  zile varianta gb gb stocare achizitionat pret ...\n",
       "4      0  senzorul luminozitate nu funcționează ecranul ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import data:\n",
    "data = pd.read_csv('Data/CleanDataset_2Classes_No_Stemming.csv')\n",
    "#sterge prima coloana\n",
    "data = data.iloc[: , 1:]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1279028",
   "metadata": {},
   "source": [
    "### Impartirea setului de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1060c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imparte setul de date: 80% - 20% \n",
    "contents = data['review_content']\n",
    "labels = data['label']\n",
    "x_train, x_test, y_train, y_test = train_test_split(contents, labels, train_size=0.8, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6433b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (6450,)\n",
      "y_test.shape : (2150,)\n"
     ]
    }
   ],
   "source": [
    "### Label encoding\n",
    "def encode_label(label):\n",
    "    if label == 0:\n",
    "        return pd.to_numeric('0.0',downcast='float')\n",
    "    elif label == 2:\n",
    "        return pd.to_numeric('1.0',downcast='float')\n",
    "\n",
    "y_train = np.stack(np.vectorize(encode_label)(y_train))\n",
    "y_test = np.stack(np.vectorize(encode_label)(y_test))\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"y_test.shape :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c73ee383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at racai/distilbert-base-romanian-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at racai/distilbert-base-romanian-cased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "### Tokenization\n",
    "import torch.nn as nn\n",
    "\n",
    "# first get the bert model\n",
    "model_name = \"racai/distilbert-base-romanian-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14ebca76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in x_train]\n",
    "pd.Series(seq_len).hist(bins = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022bb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode data in batches - takes tokenizer/texts/batch_size/maxlen => returns input_ids/attention_mask\n",
    "maxlen=400\n",
    "def batch_encode(tokenizer, texts, batch_size=256, max_length=maxlen):    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer.batch_encode_plus(batch,\n",
    "                                             max_length=maxlen,\n",
    "                                             pad_to_max_length='maxlen', \n",
    "                                             truncation=True,\n",
    "                                             return_attention_mask=True,\n",
    "                                             return_token_type_ids=False)\n",
    "        input_ids.extend(inputs['input_ids'])\n",
    "        attention_mask.extend(inputs['attention_mask'])\n",
    "    return torch.tensor(input_ids), torch.tensor(attention_mask)\n",
    "#x_train\n",
    "x_train_ids, x_train_attention = batch_encode(tokenizer, x_train.tolist())\n",
    "#x_valid\n",
    "x_valid_ids, x_valid_attention = batch_encode(tokenizer, x_valid.tolist())\n",
    "#x_test\n",
    "x_test_ids, x_test_attention = batch_encode(tokenizer, x_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ffab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model\n",
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "#dropout = 0.1\n",
    "#attention_dropout = 0.1\n",
    " \n",
    "# Configure DistilBERT's initialization\n",
    "config = DistilBertConfig()#(dropout=dropout, attention_dropout=attention_dropout, output_hidden_states=True)\n",
    "                          \n",
    "# The bare, pre-trained DistilBERT transformer model outputting raw hidden-states \n",
    "# and without any specific head on top.\n",
    "distilBERT = TFDistilBertModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Make DistilBERT layers untrainable\n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = False\n",
    "#because DistilBERT's pre-trained weights will serve as the basis for our model\n",
    "  #we wish to conserve and prevent them from updating during the initial stages of training\n",
    "    \n",
    "#ur job to add ur own classification head during the fine-tuning process in order to help the model distinguish between reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0909f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classification head\n",
    "dropout = 0.2\n",
    "lr = 3e-5\n",
    "random_state = 42\n",
    " \n",
    "def build_model(transformer, max_length=maxlen): #takes transf(the model obj) and maxlen(max tokens encoded) \n",
    "                                                    # => returns model with the classif layer\n",
    "    \n",
    "    #random seed to ensure reproducibility\n",
    "    weight_initializer = tf.keras.initializers.GlorotNormal(seed=random_state) \n",
    "    \n",
    "    #input layers\n",
    "    input_ids_layer = Input(shape=(max_length,),\n",
    "                            name='input_ids',\n",
    "                            dtype='int32')\n",
    "    input_attention_layer = Input(shape=(max_length,),\n",
    "                                  name='input_attention',\n",
    "                                  dtype='int32')\n",
    "    \n",
    "    # DistilBERT outputs a tuple where the first element at index 0\n",
    "    # represents the hidden-state at the output of the model's last layer.\n",
    "    # It is a tensor of shape (batch_size, sequence_length, hidden_size=768).\n",
    "    last_hidden_state = transformer([input_ids_layer, input_attention_layer])[0]\n",
    "    \n",
    "    # only care about DistilBERT's output for the [CLS] token, \n",
    "    # which is located at index 0 of every encoded sequence.  \n",
    "    # Splicing out the [CLS] tokens gives us 2D data.\n",
    "    cls_token = last_hidden_state[:, 0, :]\n",
    "    \n",
    "###### Define additional dropout and dense layers here \n",
    "    \n",
    "    # single node that makes up the output layer (for binary classification)\n",
    "    output = Dense(1, \n",
    "                   activation='sigmoid',\n",
    "                   kernel_initializer=weight_initializer,\n",
    "                   kernel_constraint=None,\n",
    "                   bias_initializer='zeros')(cls_token)\n",
    "    \n",
    "    # define the model\n",
    "    model = Model([input_ids_layer, input_attention_layer], output)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizers.AdamW(lr=lr), #distilbert-ro-used values\n",
    "                  loss=BCELoss(),\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea3a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training layer weights\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "steps = len(x_train.index) // batch_size\n",
    "\n",
    "# Train the model\n",
    "history = model.fit([x_train_ids, x_train_attention],\n",
    "                    y_train.to_numpy(),\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    steps_per_epoch = steps,\n",
    "                    validation_data = ([x_val_ids, x_val_attention], y_val.to_numpy()),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see how's until now and eventually add a new droptout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fine-tuning and training all weights\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "steps = len(x_train.index)\n",
    "\n",
    "# Unfreeze distilBERT layers and make available for training\n",
    "for layer in distilBERT.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# Recompile model after unfreezing\n",
    "model.compile(optimizer= AdamW(lr=2e-5), \n",
    "              loss=BCELoss(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history2 = model.fit([X_train_ids, X_train_attention],\n",
    "                           y_train.to_numpy(),\n",
    "                           epochs = epochs,\n",
    "                           batch_size = batch_size,\n",
    "                           steps_per_epoch = steps,\n",
    "                           validation_data = ([x_val_ids, x_val_attention], y_val.to_numpy()),\n",
    "                           verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac4f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb934ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3edd6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbe26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e8ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bc7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3f537b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e22455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d42c5005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize in batches:\n",
    "maxlen=400\n",
    "#train data\n",
    "train_tokens = tokenizer.batch_encode_plus(x_train.tolist(),\n",
    "                                           max_length = maxlen,\n",
    "                                           pad_to_max_length='max_length',\n",
    "                                           truncation=True,\n",
    "                                           return_token_type_ids=False\n",
    "                                           )\n",
    "#val data\n",
    "val_tokens = tokenizer.batch_encode_plus(x_val.tolist(),\n",
    "                                         max_length = maxlen,\n",
    "                                         pad_to_max_length='max_length',\n",
    "                                         truncation=True,\n",
    "                                         return_token_type_ids=False\n",
    "                                         )\n",
    "#test data\n",
    "test_tokens = tokenizer.batch_encode_plus(x_test.tolist(),\n",
    "                                          max_length = maxlen,\n",
    "                                          pad_to_max_length='max_length',\n",
    "                                          truncation=True,\n",
    "                                          return_token_type_ids=False\n",
    "                                          )\n",
    "\n",
    "## convert to tensors\n",
    "train_seq = torch.tensor(train_tokens['input_ids'])\n",
    "train_mask = torch.tensor(train_tokens['attention_mask'])\n",
    "# change from a list of 1 & 0 to array then to tensor\n",
    "y_train_tensor = torch.tensor(np.array(y_train))  \n",
    "\n",
    "val_seq = torch.tensor(val_tokens['input_ids'])\n",
    "val_mask = torch.tensor(val_tokens['attention_mask'])\n",
    "# change from a list of 1 & 0 to array then to tensor\n",
    "y_val_tensor = torch.tensor(np.array(y_val))\n",
    "\n",
    "test_seq = torch.tensor(test_tokens['input_ids'])\n",
    "test_mask = torch.tensor(test_tokens['attention_mask'])\n",
    "# change from a list of 1 & 0 to array then to tensor\n",
    "y_test_tensor = torch.tensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bfc8740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.,  ..., 1., 1., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d2a5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, y_train_tensor)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, y_val_tensor)\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# wrap tensors\n",
    "test_data = TensorDataset(test_seq, test_mask, y_test_tensor)\n",
    "test_sampler = SequentialSampler(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5e9fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              sampler=train_sampler, \n",
    "                              batch_size=batch_size\n",
    "                              )\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, \n",
    "                            sampler = val_sampler, \n",
    "                            batch_size=batch_size\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3569f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\Tensorflow\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up the optimizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_scheduler\n",
    "lr = 3e-5\n",
    "optimizer = AdamW(model.parameters(),lr = lr)\n",
    "\n",
    "#\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader)*epochs\n",
    "#rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # default value\n",
    "                                            num_training_steps = total_steps)\n",
    "#loss function\n",
    "loss= nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f35bdf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc measurement\n",
    "def accuracy_thresh(y_pred, y_true, thresh:float=0.4, sigmoid:bool=True):\n",
    "    if sigmoid:\n",
    "        y_pred = y_pred.sigmoid()\n",
    "        return np.mean(((y_pred > thresh).float() == y_true.float()).float().cpu().numpy(), axis=1).sum()\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a13256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    202.    Elapsed: 0:00:00.\n",
      "  Batch    80  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   120  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   160  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   200  of    202.    Elapsed: 0:00:00.\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    202.    Elapsed: 0:00:00.\n",
      "  Batch    80  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   120  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   160  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   200  of    202.    Elapsed: 0:00:00.\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    40  of    202.    Elapsed: 0:00:00.\n",
      "  Batch    80  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   120  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   160  of    202.    Elapsed: 0:00:00.\n",
      "  Batch   200  of    202.    Elapsed: 0:00:00.\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "import time, datetime\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch:\n",
    "for epoch_i in range(0, epochs):\n",
    "# ====================================================================\n",
    "#                               Training\n",
    "# ====================================================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss & accuracy for this epoch.\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data:\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            #store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch:\n",
    "for epoch_i in range(0, epochs):\n",
    "# =========================================================================\n",
    "#                               Training\n",
    "# =========================================================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss & accuracy for this epoch.\n",
    "    total_train_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data:\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the `to` method.\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass. \n",
    "        # PyTorch doesn't do this automatically because accumulating the gradients is \"convenient while training RNNs\". \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # It returns different numbers of parameters depending on what arguments are given and what flags are set. \n",
    "        # For our useage here, it returns the loss (because we provided labels) and the \"logits\"--the model outputs prior to activation.\n",
    "        output = model(b_input_ids, \n",
    "                       # token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       #labels=b_labels\n",
    "                       )\n",
    "        logits = output.logits\n",
    "        loss = loss(logits, b_labels.float())\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "        total_train_accuracy += accuracy_thresh(logits,b_labels.float())\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    # avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "    # print(\"  Accuracy: {0:.5f}\".format(avg_train_accuracy))\n",
    "    train_accuracy = total_train_accuracy / len(train_df)\n",
    "    print(\"  Accuracy: {0:.5f}\".format(train_accuracy))\n",
    "    \n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "# ==========================================================================\n",
    "#                               Validation\n",
    "# ==========================================================================\n",
    "# After the completion of each training epoch, measure our performance on our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            output = model(b_input_ids, \n",
    "                           # token_type_ids=None, \n",
    "                           attention_mask=b_input_mask, \n",
    "                           # labels=b_labels\n",
    "                           )\n",
    "            \n",
    "            logits = output.logits\n",
    "            loss = loss(logits, b_labels.float())\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "        # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
    "        total_eval_accuracy += accuracy_thresh(logits,b_labels.float())\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().numpy() # maybe irrelevant over here\n",
    "        label_ids = b_labels.numpy() # maybe irrelevant over here\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    # avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    # print(\"  Accuracy: {0:.5f}\".format(avg_val_accuracy))\n",
    "    val_accuracy = total_eval_accuracy / len(val_df)\n",
    "    print(\"  Accuracy: {0:.5f}\".format(val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            # 'Training Accur': avg_train_accuracy,\n",
    "            # 'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Accur': train_accuracy,\n",
    "            'Valid. Accur.': val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007836d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577515da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb7b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de90c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cf6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "maxlen = 400\n",
    "#training \n",
    "training_batch = tokenizer(x_train.tolist(),\n",
    "                           add_special_tokens=True,\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=maxlen,\n",
    "                           return_tensors='pt')\n",
    "train_data = TensorDataset(training_batch['input_ids'], \n",
    "                           training_batch['attention_mask'],\n",
    "                           torch.tensor(y_train.tolist()))\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "#valdation\n",
    "val_batch = tokenizer(x_val.tolist(),\n",
    "                      add_special_tokens=True,\n",
    "                      padding=True,\n",
    "                      truncation=True,\n",
    "                      max_length=maxlen,\n",
    "                      return_tensors='pt')\n",
    "val_data = TensorDataset(val_batch['input_ids'], \n",
    "                         val_batch['attention_mask'],\n",
    "                         torch.tensor(y_val.tolist()))\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
